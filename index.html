<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="FlexMDM: A masked diffusion model designed to generate sequences of flexible length while retaining any-order inference capabilities.">
  <meta name="keywords" content="FlexMDM">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Flexible-Length Any-Order Masked Diffusion</title>

  <!-- MathJax for LaTeX rendering -->
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']]
      }
    };
    
  </script>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Any-Order Flexible Length Masked Diffusion</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://jaeyeonkim01.github.io">Jaeyeon Kim</a><sup>1,*</sup>,</span>
            <span class="author-block">
              <a href="https://brianlck.github.io">Lee Cheuk-Kit</a><sup>1,2,*</sup>,</span>
            <span class="author-block">
              <a href="https://cdenrich.github.io">Carles Domingo-Enrich</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://yilundu.github.io">Yilun Du</a><sup>1,2</sup>,
            </span>
            <span class="author-block">
              <a href="https://shamulent.github.io">Sham Kakade</a><sup>1,2</sup>,
            </span>
            <span class="author-block">
              <a href="https://kempnerinstitute.harvard.edu/people/our-people/timothy-ngotiaoco/">Timothy Ngotiaoco</a><sup>1,2</sup>,
            </span>
            <span class="author-block">
              <a href="https://sitanchen.com">Sitan Chen</a><sup>1,✝</sup>
            </span>
            <span class="author-block">
              <a href="http://malbergo.me">Michael Albergo</a><sup>1,2,4,✝</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Harvard University</span>
            <span class="author-block"><sup>2</sup>Kempner Institute</span>
            <span class="author-block"><sup>3</sup>Microsoft Research New England</span>
            <span class="author-block"><sup>4</sup>IAIFI MIT</span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>*</sup>Equal contribution, randomized ordering</span>
            <span class="author-block"><sup>✝</sup>Lead senior authors</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2509.01025"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/brianlck/interpretable-flow"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img id="teaser" src="./static/videos/flexmdm.gif" style="width: 100%; margin-top: -50px; height: auto;" />
      <h2 class="subtitle has-text-centered">
        <span class="dnerf"> FlexMDM </span> generates variable-length sequences by inserting masks and unmasking them arbitrarily
      </h2>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Introduction</h2>
        <div class="content has-text-justified">
          <p> Masked Diffusion Models (MDM) have been a promising alternative to autoregressive models, with recent scaling-up efforts, e.g., LLaDA, Dream7B, Mercury, Gemini Diffusion. Despite these successes, they struggle to (1) model variable-length sequence distributions and (2) insert new tokens during generation. </p>
          <p> FlexMDM addresses these issues: By baking in the ability to insert mask tokens, FlexMDM exhibits superior performance over masked diffusion in planning and reasoning tasks where length flexibility is crucial. </p>
          <p> FlexMDM scales up to 8B parameters—we retrofit LLaDA, an 8B open-sourced pretrained MDM, and fine-tune it using 1000 H100 GPU hours. FlexMDM’s ability to insert new tokens in arbitrary positions enables better reasoning capabilities on real-world reasoning tasks such as
math (GSM8K) and code infilling (Humaneval-infill). Compared to MDM, FlexMDM achieves superior performance on math (GSM8K, 58%→67%) and code infilling (52%→65%) </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
</section>


<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Modeling length is beneficial for generation</h2> 
    <div class="content has-text-justified">
      Masked diffusion models assume a fixed-length generation window, yet languages are inherently variable-length. By incorporating the ability to insert mask tokens, FlexMDM exhibits superior scaling behavior over masked diffusion in planning and reasoning tasks where length flexibility is crucial.
    </div>
  <div class="container is-max-desktop">
    <div class="is-centered">

      <!-- Reasoning. -->
        <div class="content has-text-centered">
          <h2 class="is-4">Reasoning</h2>
          <img src="./static/images/reasoning.png" width="80%"/>
          <p>
            We retrofit an 8B parameter LLaDA model into FlexMDM using 1000 H100 GPU hours. By allowing the model to insert new tokens in arbitrary positions, FlexMDM exhibits better reasoning capabilities on real-world reasoning tasks such as coding and grade school mathematics.
          </p>
        </div>
      <!--/ Reasoning. -->

      <!-- Planning -->
      <div class="content has-text-centered">
        <h2 class="is-3">Planning</h2>
        <div class="columns">
          <div class="column is-half">
            <img src="./static/images/subgoal.png" height="30%"/>
          </div>
          <div id="column is-half">
            <table class="fancy-table" style="margin-top: 50px;">
            <thead>
            <tr>
            <th>Difficulty</th>
            <th>MDM</th>
            <th>FlexMDM</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>Easy</td>
            <td>68.4%</td>
            <td><strong>92.3%</strong></td>
            </tr>
            <tr>
            <td>Medium</td>
            <td>29.3%</td>
            <td><strong>90.4%</strong></td>
            </tr>
            <tr>
            <td>Hard</td>
            <td>24.2%</td>
            <td><strong>90.0%</strong></td>
            </tr>
            </tbody>
            <caption>Table 1: FlexMDM outperforms MDM on the subgoal-style maze-planning task.</caption>
            </table>
            <p>
              Trajectories in planning tasks are inherently variable-length. By allowing the model to insert new tokens in arbitrary positions, FlexMDM is able to generate more trajectories that pass through subgoals in a valid way.
            </p>
          </div>
        </div>
      </div>
      <!--/ Planning -->
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
        <h2 class="title is-3">Modeling with Stochastic Interpolants</h2>
      <div class="content container is-max-desktop">
        <div class="has-text-centered" style="margin-bottom: 20px">
        <video id="" autoplay controls muted loop playsinline width="80%">
          <source src="./static/videos/FlexMDMInterpolant.mp4"
                  type="video/mp4">
        </video>
        </div>
        <p> The modeling of the method relies on joint interpolants, an extension of the stochastic interpolant. In essence, the interpolant framework facilitates the construction of generative models by defining a collection of time-indexed random variables that govern the marginal probability of the generation at time t and describe the mathematical quantities required for generation. </p>

        <p> In the context of FlexMDM, we bridge between a point mass at an empty sequence and data distribution $p_1$. The intermediate distribution follows the distribution of $x_t$, which we define per dimension by sampling an insertion time $T_1$ and unmasking time $T_2$ : </p>
        $$
          \quad T_2^i \sim \mathbf{1}_{\{t \ge T_1^i\}}\frac{\dot{\beta_t}}{1-\beta_{T_1}} \; dt, \quad  x_t^i = \begin{cases}
        \text{(empty)}, & 0 < t < T_1^i \\
        \mathbf{m}, & T_1^i \leq t <  T_2^i \\
        x_1^i, &   T_2^i \leq t \leq 1
    \end{cases}
        $$
      </p>
      <p>
        This is augmented by an auxiliary variable defined in a way that is coupled with $x_t$:
        $$
            s_t \colon= \{i \in \{0,\dots,\mathrm{len}(x_1)-1\} \mid T_1^i \le t\},
        $$
        which is the set of indices that have been inserted by time $t$. Jointly with $x_t$, they characterize the quantities needed to define the inference procedure:
        <ul style="margin-left: 0; padding-left: 40px; margin-top: 20px;">
        <li style="margin-bottom: 15px;">
          <strong style="color: #0066cc;">Unmasking posterior</strong> (modeled by <span style="color: #0066cc;">$f_\theta(x,t)[i] \in \Delta(\Sigma)$</span>): for each index $i$ that $x^i=\mathbf{m}$, the posterior distribution over the underlying clean token.
          <br>
          <span style="color: #0066cc;">$$f_\theta(x, t)[i, v] = \mathbb{P} (x_1^{s_t[i]} = v | x_t = x)$$</span>
        </li>
        <li style="margin-bottom: 15px;">
          <strong style="color: #009900;">Insertion expectation</strong> (modeled by <span style="color: #009900;">$g_\theta(x,t)[i] \in \mathbb{R}_{\geq 0}$</span>): for all indices $i$ in $x$, the expected number of tokens that remain to be inserted in between $x^{i-1}$ and $x^{i}$.
          <br>
          <span style="color: #009900;">$$g_\theta(x, t)[i] = \mathbb{E}[s_t[i] - s_t[i-1] - 1 | x_t = x]$$</span>
        </li>
      </ul>
      </p>
      </div>
      </div>
    </section>

<section class="section">
  <div class="content container is-max-desktop">
    <h2 class="title is-3">Variable Length Masked Diffusions: Inference</h2>
    
    <p>We outline two inference algorithms for FlexMDM: <strong style="color: #800080;">vanilla inference</strong> and <strong style="color: #800080;">adaptive inference</strong>.</p>
    
    <p><strong>Vanilla inference.</strong> We discretize the CTMC using trained neural networks $(f_\theta,g_\theta)$ with $\tau$-leaping. At each step, we simultaneously:</p>
    
    <ul style="margin-left: 20px;">
      <li><strong style="color: #0066cc;">Unmasking:</strong> For each mask token, sample unmasking events according to a rate matrix that governs evolution of the process.</li>
      <li><strong style="color: #009900;">Insertion:</strong> Sample mask-token insertions from a Poisson distribution parameterized by the insertion rate.</li>
    </ul>
    
    <p><strong>Adaptive inference.</strong> We can choose unmasking positions <strong style="color: #800080;">adaptively</strong>, prioritizing <strong style="color: #800080;">the most confident indices</strong> based on the model's unmasking posterior or semi-autoregressive rules. This substantially boosts performance.</p>
    
    <div style="background-color: #f8f9fa; padding: 15px; border-left: 4px solid #0066cc; margin: 20px 0;">
      <p><strong>Key Result:</strong> Any sampling scheme that (i) unmasks arbitrary subsets but draws tokens from the ground-truth unmasking posterior and (ii) applies insertion CTMC governed by the ground-truth rate matrix samples from the target distribution $p_1$.</p>
    </div>

    This is summarized in the following pseudocode:

    <div class="columns is-desktop" style="margin-top: 20px;">
        <!-- Container for Subroutine 1 -->
        <div class="column">
            <h2 class="title is-4">Subroutine 1: VLMDM inference</h2>
            <div class="font-code content">
                <p><strong>Require:</strong> Learned functions ($f_{\theta}, g_{\theta}$)</p>
                <p><strong>Require:</strong> Discretization $0 = t_1 < \dots < t_N = 1$</p>
                <p><strong>Require:</strong> Insertion, Unmasking schedule $\alpha_t, \beta_t$</p>
                <ol>
                    <li><span>Initialize $X_{t_1} \leftarrow \epsilon$</span></li>
                    <li><span><span class="keyword">for</span> k = 1 <span class="keyword">to</span> N &minus; 1</span></li>
                    <li style="padding-left: 2em;"><span>$\tau \leftarrow t_{k+1} - t_k$</span></li>
                    <li style="padding-left: 2em;"><span class="highlight">Invoke Subroutine 2 for unmasking</span></li>
                    <li style="padding-left: 2em;"><span><span class="keyword">for</span> i <span class="keyword">in</span> [len($X_{t_k}$)]</span></li>
                    <li style="padding-left: 4em;"><span>Set rate $r \leftarrow \alpha_{t_k} / (1 - \alpha_{t_k}) \cdot \tau$</span></li>
                    <li style="padding-left: 4em;"><span>Sample $l \sim \text{Poi}(r \cdot g_{\theta}(X_{t_k}, t_k)[i])$</span></li>
                    <li style="padding-left: 4em;"><span class="highlight">Insert $l$ masks between $X_{t_k}^{i-1}$ and $X_{t_k}^i$</span></li>
                    <li><span><span class="keyword">return</span> $X_{t_N}$</span></li>
                </ol>
            </div>
        </div>

        <!-- Container for Subroutine 2 -->
        <div class="column">
            <h2 class="title is-4">Subroutine 2: Unmasking Step</h2>
            <div class="font-code content">
                <ol>
                    <li><span><span class="keyword">if</span> <span class="highlight">vanilla inference</span> :</span></li>
                    <li style="padding-left: 2em;"><span><span class="keyword">for</span> i &isin; {i | $X_{t_k}^i$ = m} and v &isin; &Sigma;</span></li>
                    <li style="padding-left: 4em;"><span>Set rate $r \leftarrow \beta_{t_k} / (1 - \beta_{t_k}) \cdot \tau$</span></li>
                    <li style="padding-left: 4em;"><span>$k_v \sim \text{Poi}(r \cdot f_{\theta}(X_{t_k}, t_k)[i, v])$</span></li>
                    <li style="padding-left: 4em;"><span><span class="keyword">if</span> &exist;v such that $k_v = 1$</span></li>
                    <li style="padding-left: 6em;"><span>Set $X_{t_k}^i \leftarrow v$</span></li>
                    <li><span><span class="keyword">if</span> <span class="highlight">adaptive inference</span> :</span></li>
                    <li style="padding-left: 2em;"><span>Select K (the size of |S'|)</span></li>
                    <li style="padding-left: 2em;"><span><span class="keyword">for</span> i &isin; {i | $X_{t_k}^i$ = m}</span></li>
                    <li style="padding-left: 4em;"><span>Compute confidence $C^i$</span></li>
                    <li style="padding-left: 2em;"><span><span class="keyword">for</span> i <span class="keyword">in</span> argmaxK(C)</span></li>
                    <li style="padding-left: 4em;"><span>$X_{t_k}^i \sim \text{Cat}(f_{\theta}(X_{t_k}, t_k)[i])$</span></li>
                </ol>
            </div>
        </div>
    </div>
    </div>

  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{kim2025any,
  title={Any-Order Flexible Length Masked Diffusion},
  author={Kim, Jaeyeon and Cheuk-Kit, Lee and Domingo-Enrich, Carles and Du, Yilun and Kakade, Sham and Ngotiaoco, Timothy and Chen, Sitan and Albergo, Michael},
  journal={arXiv preprint arXiv:2509.01025},
  year={2025}
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This webpage is adapted from <a
              href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
